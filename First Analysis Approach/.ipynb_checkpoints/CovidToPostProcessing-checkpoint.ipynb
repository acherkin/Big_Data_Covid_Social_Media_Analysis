{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e727a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b4658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkSession.builder.master(\"local[1]\").appName(\"spark_tweets\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a22b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1ba828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375fdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid test data schema\n",
    "schema = StructType([StructField(\"Entity\", StringType()), StructField(\"ISO code\", StringType()), StructField(\"Date\", StringType()), StructField(\"Source URL\", StringType()), StructField(\"Source label\", StringType()), StructField(\"Notes\", StringType()),StructField(\"Cumulative total\", FloatType()),StructField(\"Daily change in cumulative total\", FloatType()),StructField(\"Cumulative total per thousand\", FloatType()),StructField(\"Daily change in cumulative total per thousand\", FloatType()),StructField(\"7-day smoothed daily change\", FloatType()),StructField(\"7-day smoothed daily change per thousand\", FloatType()),StructField(\"Short-term positive rate\", FloatType()),StructField(\"Short-term tests per case\", FloatType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load covid test data\n",
    "df_test = spark.read.csv(\"hdfs://namenode:9000/data/covid-testing-all-observations.csv\", schema)\n",
    "df_test.show()\n",
    "\n",
    "#select needed columns\n",
    "df_test2 = df_test.select(col(\"Date\"),col(\"Short-term positive rate\"))\n",
    "#clean data\n",
    "df_test2 =df_test2.na.drop(subset=[\"Short-term positive rate\"])\n",
    "df_test2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load covid hospitalization data\n",
    "schema_h = StructType([StructField(\"entity\", StringType()),StructField(\"iso_code\", StringType()),StructField(\"date\", StringType()),StructField(\"indicator\", StringType()), StructField(\"value\", FloatType())])\n",
    "df_h = spark.read.csv(\"hdfs://namenode:9000/data/covid-hospitalizations.csv\", schema_h)\n",
    "df_h.na\n",
    "df_h =df_h.na.drop(subset=[\"value\"])\n",
    "df_h.show()\n",
    "\n",
    "\n",
    "#filter for week data\n",
    "df_hw = df_h.filter(col(\"indicator\").contains(\"Week\"))\n",
    "#per mission rate\n",
    "df_hw_mil = df_hw.filter(col(\"indicator\").contains(\"mill\"))\n",
    "df_hw = df_hw.exceptAll(df_hw_mil)\n",
    "\n",
    "df_hw_mil.show()\n",
    "df_hw.show()\n",
    "\n",
    "#filter daily data\n",
    "df_hd = df_h.filter(col(\"indicator\").contains(\"Daily\"))\n",
    "#per million rate\n",
    "df_hd_mil = df_hd.filter(col(\"indicator\").contains(\"mill\"))\n",
    "df_hd = df_hd.exceptAll(df_hd_mil)\n",
    "\n",
    "df_hd_mil.show()\n",
    "df_hd.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19aaa5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "java.net.UnknownHostException: namenode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-263edd0f1805>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_tw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hdfs://namenode:9000/data/tweets\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mschema_tweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: java.net.UnknownHostException: namenode"
     ]
    }
   ],
   "source": [
    "schema_tweet = StructType([StructField(\"ID\", StringType()), StructField(\"date\", StringType()), StructField(\"time\", StringType())])\n",
    "\n",
    "df_tw = spark.read.csv(\"hdfs://namenode:9000/data/tweets\", sep=r'\\t', schema= schema_tweet, header=True)\n",
    "\n",
    "df_tw_c = df_tw.groupBy(\"date\").count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37803391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf426780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tweet data\n",
    "schema_c = StructType([StructField(\"date\", StringType()), StructField(\"count\", IntegerType())])\n",
    "\n",
    "df_count = spark.read.csv(\"hdfs://namenode:9000/data/full_count.csv\", schema_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74667ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate average for the per million daily hospitalization rate\n",
    "df_hd_MA = df_hd_mil.groupBy(\"date\").agg(mean('value'))\n",
    "#filter for relevant dates\n",
    "df_hd_MA = df_hd_MA.filter(col(\"date\").contains(\"2020\"))\n",
    "df_hd_MA.show()\n",
    "\n",
    "#calculate average for the  daily hospitalization rate\n",
    "df_hd_A = df_hd.groupBy(\"date\").agg(mean('value'))\n",
    "df_hd_A = df_hd_A.filter(col(\"date\").contains(\"2020\"))\n",
    "\n",
    "#calculate average for the per million weekly hospitalization rate\n",
    "df_hw_MA = df_hw_mil.groupBy(\"date\").agg(mean('value'))\n",
    "df_hw_MA = df_hw_MA.filter(col(\"date\").contains(\"2020\"))\n",
    "\n",
    "#calculate average for the  weekly hospitalization rate\n",
    "df_hw_A = df_hw.groupBy(\"date\").agg(mean('value'))\n",
    "df_hw_A = df_hw_A.filter(col(\"date\").contains(\"2020\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8369651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process test data, make average\n",
    "df_test2020 = df_test2.filter(col(\"Date\").contains(\"2020\"))\n",
    "\n",
    "df_testA = df_test2020.groupBy(\"Date\").agg(mean('Short-term positive rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e01547",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-adea2b8e44a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_tweet_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_testA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_testA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m==\u001b[0m \u001b[0mdf_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_count' is not defined"
     ]
    }
   ],
   "source": [
    "#Save test data table\n",
    "df_tweet_test = df_count.join(df_testA, df_testA.Date== df_count.date, 'inner').select(df_count.date, col(\"count\"), col(\"avg(Short-term positive rate)\").alias(\"value\"))\n",
    "df_tweet_test.show()\n",
    "df_tweet_test.write.csv(\"hdfs://namenode:9000/data/tweet_tests.csv\")\n",
    "\n",
    "\n",
    "#Save hospitalization per million data table\n",
    "df_hd_MA2 = df_count.join(df_hd_MA, df_hd_MA.date== df_count.date, 'inner').select(df_count.date, col(\"count\"), col(\"avg(value)\").alias(\"value\"))\n",
    "df_hd_MA2.show()\n",
    "df_hd_MA2.write.csv(\"hdfs://namenode:9000/data/tweet_day_mil.csv\")\n",
    "\n",
    "#Save hospitalization data table\n",
    "df_hd_A2 = df_count.join(df_hd_A, df_hd_A.date== df_count.date, 'inner').select(df_count.date, col(\"count\"), col(\"avg(value)\").alias(\"value\"))\n",
    "df_hd_A2.show()\n",
    "df_hd_A2.write.csv(\"hdfs://namenode:9000/data/tweet_day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111400ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df_tweet_test.corr(\"tweets\", \"avr_test_pos_rate\")\n",
    "print( correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da81b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis for test tweet data\n",
    "df_tweet_test = df_tweet_test.sort('date')\n",
    "df_tweet_test =df_tweet_test.filter(col(\"avr_test_pos_rate\")<5)\n",
    "df_tweet_test.show()\n",
    "\n",
    "df_t_ = df_tweet_test.toPandas()\n",
    "\n",
    "df_t_.plot(x='date', y='tweets', kind='bar')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t_.plot(x='date', y='avr_test_pos_rate', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f945072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis for daily hospitalizations tweet data\n",
    "correlationH = df_hd_A2.corr(\"tweets\", \"hospitalizations\")\n",
    "print( correlationH)\n",
    "df_hd_A2 = df_hd_A2.sort('date')\n",
    "\n",
    "df_hPD = df_hd_A2.toPandas()\n",
    "df_hPD.plot(x='date', y='hospitalizations', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934762a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_hd_MA2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-85e5e8302887>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorrelationHM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_hd_MA2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tweets\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"hospitalizations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcorrelationHM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_hd_MA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_hd_MA2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_hMPD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_hd_MA2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_hMPD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hospitalizations'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_hd_MA2' is not defined"
     ]
    }
   ],
   "source": [
    "#analysis for hospitalizations per million tweet data\n",
    "correlationHM = df_hd_MA2.corr(\"tweets\", \"hospitalizations\")\n",
    "print( correlationHM)\n",
    "df_hd_MA2 = df_hd_MA2.sort('date')\n",
    "df_hMPD = df_hd_MA2.toPandas()\n",
    "df_hMPD.plot(x='date', y='hospitalizations', kind='bar')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
